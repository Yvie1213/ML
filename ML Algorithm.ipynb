{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is: 2900.1936284934814\n",
      "R2 score is: 0.4526027629719195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE is:\", mse)\n",
    "print(\"R2 score is:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "Mean Squared Error (MSE): Measures the average of the squares of the errors. Lower values are better.\n",
    "R-squared: Represents the percentage of the dependent variable’s variation that can be predicted based on the independent variables. Closer to 1 is better.\n",
    "\n",
    "Applying with Sci-kit Learn\n",
    "1. Get the Diabetes Dataset loaded: Ten baseline variables, including age, sex, BMI, average blood pressure, and six blood serum measures for diabetic patients, are included in this dataset.\n",
    "2. Split the Dataset: Divide it into training and testing sets.\n",
    "3. Create and Train the Linear Regression Model: Build the model using the training set.\n",
    "4. Predict and Evaluate: Use the test set to make predictions and then evaluate the model using MSE and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n",
      "Precision: 0.9459459459459459\n",
      "Recall: 0.9859154929577465\n",
      "F1 Score: 0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "Accuracy: Accuracy is the ratio of correctly predicted observations to total observations.\n",
    "Precision and Recall: Precision is the ratio of correctly predicted positive observations to all expected positive observations. Recall is the proportion of correctly predicted positive observations to all observations made in the actual class.\n",
    "F1 Score: An equilibrium between recall and precision.\n",
    "\n",
    "Here are the steps we’ll follow to apply logistic regression.\n",
    "1. Load the Breast Cancer Dataset: This dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, and the goal is to classify them as benign or malignant.\n",
    "2. Split the Dataset: Divide it into training and testing sets.\n",
    "3. Create and Train the Logistic Regression Model: Build the model using the training set.\n",
    "4. Predict and Evaluate: Use the test set to make predictions and then evaluate the model using Accuracy, Precision, Recall, and F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9444444444444444\n",
      "Precision: 0.953968253968254\n",
      "Recall: 0.9345238095238096\n",
      "F1 Score: 0.9424740010946907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "For classification: Accuracy, precision, recall, and F1 score.\n",
    "For Regression: Mean Squared Error (MSE), R-squared.\n",
    "\n",
    "Here are the steps we’ll follow in the code below.\n",
    "1. Load the Wine Dataset\n",
    "2. Split the Dataset\n",
    "There are training and testing sets inside the dataset. This is done to train the model on one part of the data (training set) and test its performance on unseen data (testing set). We used 80% of the data for training and 20% for testing.\n",
    "3. Create and Train the Decision Tree Model\n",
    "A Decision Tree Classifier is created. This model will learn from the training data. It builds a tree-like model of decisions, where each node in the tree represents a feature of the dataset, and the branches represent decision rules, leading to different outcomes or classifications.\n",
    "4. Predict and Evaluate\n",
    "The model is used to predict the classifications of the test set. The performance of the model is then assessed by contrasting these predictions with the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9444444444444444\n",
      "Precision: 0.953968253968254\n",
      "Recall: 0.9345238095238096\n",
      "F1 Score: 0.9424740010946907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Creating and training the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='macro')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='macro')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics:\n",
    "Classification: Accuracy, Precision, Recall, F1 Score.\n",
    "Regression: Mean Squared Error (MSE), R-squared.\n",
    "\n",
    "Applying with Sci-kit Learn\n",
    "1. Create and Train the Random Forest Model:\n",
    "Initialize a Random Forest Classifier.\n",
    "Utilizing the training data, fit (train) the model.\n",
    "2. Predict:\n",
    "Use the trained model to predict the labels of the test data.\n",
    "3. Evaluate:\n",
    "Assess the model’s performance on the test data using Accuracy, Precision, Recall, and F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: 78.8556658259773\n",
      "Silhouette: 0.5511916046195919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Applying K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predicting the cluster for each data point\n",
    "y_pred_clusters = kmeans.predict(X)\n",
    "\n",
    "# Evaluating the model\n",
    "inertia = kmeans.inertia_\n",
    "silhouette = silhouette_score(X, y_pred_clusters)\n",
    "\n",
    "print(\"Inertia:\", inertia)\n",
    "print(\"Silhouette:\", silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "Inertia: The total squared distance of the samples to the nearest cluster center is known as inertia. It is better to have lower values.\n",
    "Silhouette Score: Indicates how cohesively an item belongs to its own cluster as opposed to how much it separates from other clusters. A high silhouette score means that the item is well matched to its own cluster and poorly matched to nearby clusters. The silhouette score goes from -1 to 1.\n",
    "\n",
    "Applying with Sci-kit Learn\n",
    "1. Load the Iris Dataset:\n",
    "2. Apply K-Means Clustering:\n",
    "We initialize a K-Means clustering algorithm with n_clusters=3, as there are three species of iris in the dataset. However, the algorithm is unaware of these species; it will simply try to find the best way to group the data into three clusters.\n",
    "We fit the model to the data X, which includes our four features. The K-Means algorithm iteratively assigns each data point to one of the three clusters based on the distance of the data point to the cluster centroids.\n",
    "3. Predict Clusters:\n",
    "The predict method is used to assign each data point in X to one of the three clusters. This step is somewhat conceptual with K-Means since the fitting and prediction happen together, but essentially, each data point is now labeled with a cluster number\n",
    "4. Evaluate the Clustering:\n",
    "We evaluate our clustering using two metrics:\n",
    "• Inertia: This is the sum of squared distances of samples to their closest cluster center. It’s a measure of how internally coherent clusters are. We aim for lower inertia.\n",
    "• Silhouette Score: This measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9473684210526315\n",
      "Precision: 0.961038961038961\n",
      "Recall: 0.9302325581395349\n",
      "F1 Score: 0.9422297297297297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the SVM model\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm, average='macro')\n",
    "recall_svm = recall_score(y_test, y_pred_svm, average='macro')\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
    "\n",
    "accuracy_svm, precision_svm, recall_svm, f1_svm\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "print(\"Precision:\", precision_svm)\n",
    "print(\"Recall:\", recall_svm)\n",
    "print(\"F1 Score:\", f1_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "Classification: Accuracy, Precision, Recall, F1 Score.\n",
    "Regression: Mean Squared Error (MSE), R-squared.\n",
    "\n",
    "Applying with Sci-kit Learn\n",
    "1. Create and Train the SVM Model:\n",
    "A Support Vector Machine (SVM) model is created using the default settings. SVM is known for its ability to create a hyperplane (or multiple hyperplanes in higher-dimensional spaces) that separates the classes with as wide a margin as possible.\n",
    "2. Predict:\n",
    "The trained SVM model is then used to predict the class labels of the test data. It does this by determining on which side of the hyperplane each data point falls.\n",
    "3. Evaluate:\n",
    "The model’s predictions are evaluated against the actual labels of the test set to assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8472222222222222\n",
      "Precision: 0.8649844547206135\n",
      "Recall: 0.8476479221745045\n",
      "F1 Score: 0.8437352605469787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the Naive Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics:\n",
    "Accuracy: Measures overall correctness of the model.\n",
    "Precision, Recall, and F1 Score: Especially important in cases where class distribution is imbalanced.\n",
    "\n",
    "1. Load the Digits Dataset:\n",
    "2. Split the Dataset:\n",
    "Similar to previous examples, the dataset is divided into training and testing sets. We use 80% of the data for training and 20% for testing. This helps in training the model on a large portion of the data and then evaluating its performance on a separate set that it hasn’t seen before.\n",
    "3. Create and Train the Naive Bayes Model:\n",
    "A Gaussian Naive Bayes classifier is created. This variant of Naive Bayes assumes that the continuous values associated with each feature are distributed according to a Gaussian (normal) distribution.\n",
    "The model is then trained (fitted) on the training data. It learns to associate the input features (pixel values) with the target values (digit classes).\n",
    "4. Predict and Evaluate:\n",
    "After training, the model is used to predict the class labels of the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8055555555555556\n",
      "Precision: 0.7912698412698412\n",
      "Recall: 0.7976190476190476\n",
      "F1 Score: 0.78998778998779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "precision_knn = precision_score(y_test, y_pred_knn, average='macro')\n",
    "recall_knn = recall_score(y_test, y_pred_knn, average='macro')\n",
    "f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy_knn)\n",
    "print(\"Precision:\", precision_knn)\n",
    "print(\"Recall:\", recall_knn)\n",
    "print(\"F1 Score:\", f1_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "Classification: Accuracy, Precision, Recall, F1 Score.\n",
    "Regression: Mean Squared Error (MSE), R-squared.\n",
    "\n",
    "Applying with Sci-kit Learn\n",
    "1. Create and Train the KNN Model:\n",
    "A K-Nearest Neighbors (KNN) model is created with n_neighbors=3. This means the model looks at the three nearest neighbors of a data point to make a prediction.\n",
    "The model is trained (fitted) with the training data. During training, it doesn’t build a traditional model but memorizes the dataset.\n",
    "2. Predict:\n",
    "The trained KNN model is then used to predict the class labels (types of wine) of the test data. The model determines the most common class among these neighbors for each point in the test set by examining the three nearest points in the training set.\n",
    "3. Evaluate:\n",
    "The model’s predictions are evaluated against the actual labels of the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
